{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:54:08.972256Z",
     "start_time": "2023-12-29T02:54:06.339197Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from model.GeoCLIP import GeoCLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    },
    {
     "data": {
      "text/plain": "GeoCLIP(\n  (image_encoder): ImageEncoder(\n    (CLIP): CLIPModel(\n      (text_model): CLIPTextTransformer(\n        (embeddings): CLIPTextEmbeddings(\n          (token_embedding): Embedding(49408, 768)\n          (position_embedding): Embedding(77, 768)\n        )\n        (encoder): CLIPEncoder(\n          (layers): ModuleList(\n            (0-11): 12 x CLIPEncoderLayer(\n              (self_attn): CLIPAttention(\n                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n              )\n              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (mlp): CLIPMLP(\n                (activation_fn): QuickGELUActivation()\n                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n              )\n              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      )\n      (vision_model): CLIPVisionTransformer(\n        (embeddings): CLIPVisionEmbeddings(\n          (patch_embedding): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n          (position_embedding): Embedding(257, 1024)\n        )\n        (pre_layrnorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n        (encoder): CLIPEncoder(\n          (layers): ModuleList(\n            (0-23): 24 x CLIPEncoderLayer(\n              (self_attn): CLIPAttention(\n                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n              )\n              (layer_norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (mlp): CLIPMLP(\n                (activation_fn): QuickGELUActivation()\n                (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n                (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n              )\n              (layer_norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            )\n          )\n        )\n        (post_layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      )\n      (visual_projection): Linear(in_features=1024, out_features=768, bias=False)\n      (text_projection): Linear(in_features=768, out_features=768, bias=False)\n    )\n    (mlp): Sequential(\n      (0): Linear(in_features=768, out_features=768, bias=True)\n      (1): ReLU()\n      (2): Linear(in_features=768, out_features=512, bias=True)\n    )\n  )\n  (location_encoder): LocationEncoder(\n    (LocEnc0): LocationEncoderCapsule(\n      (capsule): Sequential(\n        (0): GaussianEncoding()\n        (1): Linear(in_features=512, out_features=1024, bias=True)\n        (2): ReLU()\n        (3): Linear(in_features=1024, out_features=1024, bias=True)\n        (4): ReLU()\n        (5): Linear(in_features=1024, out_features=1024, bias=True)\n        (6): ReLU()\n      )\n      (head): Sequential(\n        (0): Linear(in_features=1024, out_features=512, bias=True)\n      )\n    )\n    (LocEnc1): LocationEncoderCapsule(\n      (capsule): Sequential(\n        (0): GaussianEncoding()\n        (1): Linear(in_features=512, out_features=1024, bias=True)\n        (2): ReLU()\n        (3): Linear(in_features=1024, out_features=1024, bias=True)\n        (4): ReLU()\n        (5): Linear(in_features=1024, out_features=1024, bias=True)\n        (6): ReLU()\n      )\n      (head): Sequential(\n        (0): Linear(in_features=1024, out_features=512, bias=True)\n      )\n    )\n    (LocEnc2): LocationEncoderCapsule(\n      (capsule): Sequential(\n        (0): GaussianEncoding()\n        (1): Linear(in_features=512, out_features=1024, bias=True)\n        (2): ReLU()\n        (3): Linear(in_features=1024, out_features=1024, bias=True)\n        (4): ReLU()\n        (5): Linear(in_features=1024, out_features=1024, bias=True)\n        (6): ReLU()\n      )\n      (head): Sequential(\n        (0): Linear(in_features=1024, out_features=512, bias=True)\n      )\n    )\n  )\n)"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GeoCLIP(gps_gallary_path='./model/gps_gallery_100K.csv')\n",
    "model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:54:15.795313Z",
     "start_time": "2023-12-29T02:54:08.971184Z"
    }
   },
   "id": "ef010967dfcc1601"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 Predictions\n",
      "=================\n",
      "Prediction 1: tensor([ 10.0018, -84.1201], dtype=torch.float64)\n",
      "Probability: 0.04395098611712456\n",
      "\n",
      "Prediction 2: tensor([  9.9996, -84.1193], dtype=torch.float64)\n",
      "Probability: 0.04127639904618263\n",
      "\n",
      "Prediction 3: tensor([ 10.0000, -84.1199], dtype=torch.float64)\n",
      "Probability: 0.04109963774681091\n",
      "\n",
      "Prediction 4: tensor([  9.9998, -84.1199], dtype=torch.float64)\n",
      "Probability: 0.04078119993209839\n",
      "\n",
      "Prediction 5: tensor([  9.9367, -84.1518], dtype=torch.float64)\n",
      "Probability: 0.03976961970329285\n"
     ]
    }
   ],
   "source": [
    "file_name = \"images/2.jpg\"\n",
    "\n",
    "image = Image.open(file_name)\n",
    "\n",
    "with torch.no_grad():\n",
    "    top_pred_gps, top_pred_prob = model.predict(image, top_k=5)\n",
    "\n",
    "# Print top 5 predictions\n",
    "print(\"Top 5 Predictions\")\n",
    "print(\"=================\")\n",
    "for i in range(5):\n",
    "    print(f\"Prediction {i+1}: {top_pred_gps[i]}\")\n",
    "    print(f\"Probability: {top_pred_prob[i]}\")\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-29T02:55:18.691720Z",
     "start_time": "2023-12-29T02:55:15.379231Z"
    }
   },
   "id": "7c9ae8ac599c5b36"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "b3b125a3423af16e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
